## Can Supervised Fine-tuning imbue length control in reasoning models?

A solution to imbue length control capabilities in reasoning models is to construct an SFT dataset by first sampling multiple responses for a prompt, retagging each response with its length, and then constructing SFT data to train a model to generate a corresponding response given the question and desired length.

However, reasoning models such as DeepSeek-R1-Distill-Qwen-1.5B, by default, show a very narrow distribution of output lengths for a particular question and do not respond to length constraints provided in the prompt. This intuitively should result in the model ignoring the length constraint provided in the prompt even after supervised fine-tuning, since for each question, the lengths on which the model will be trained are already from its natural distribution. To demonstrate this, we perform supervised fine-tuning using the above-mentioned methodology on Deepseek-R1-Distill-Qwen-1.5B and evaluate the model's length control capabilities.

## Experimental Setup

We use the same dataset as RL training in LCPO. We perform a hyperparameter search over Learning Rates: {1e-6, 1e-5, 1e-4} and Batch Sizes: {256, 512}. We use validation loss for early stopping.

## Results

The following table shows the requested tokens versus the actual tokens generated by the SFT model on the AIME2025 dataset.

| Requested Tokens | Actual Tokens |
|------------------|---------------|
| 512              | 21388         |
| 1024             | 22749         |
| 2048             | 21426         |
| 4096             | 20903         |

The results support the hypothesis that naive supervised fine-tuning on the length data generated this way would have a negligible effect on the length-control capabilities of the model. While this method has been effective in past work [1], it was applied to general tasks where a greater length would not typically lead to better performance, and where the model already had some initial capability of following length constraints.

**TL;DR:** The results highlight that SFT alone may not be enough to imbue length control capabilities in reasoning models, while LCPO (an online RL method) is highly effective.